{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T15:03:08.133051Z",
     "start_time": "2021-03-22T15:03:03.427603Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\urvi3\\anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T15:50:39.187732Z",
     "start_time": "2021-03-22T15:50:39.162805Z"
    }
   },
   "outputs": [],
   "source": [
    "class NMTDataset:\n",
    "    def __init__(self, problem_type='en-fra'):\n",
    "        self.problem_type = 'en-fra'\n",
    "        self.inp_lang_tokenizer = None\n",
    "        self.targ_lang_tokenizer = None\n",
    "\n",
    "\n",
    "    def unicode_to_ascii(self, s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def preprocess_sentence(self, w):\n",
    "        w = self.unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        #replace special characters\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "        w = w.strip()\n",
    "\n",
    "        # add a start and end token to each sentence\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "\n",
    "    def create_dataset(self, path, num_examples):\n",
    "        lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "        \n",
    "        word_pairs = []\n",
    "        for l in lines[:num_examples]:\n",
    "            curr = [self.preprocess_sentence(l.split('\\t')[0]), self.preprocess_sentence(l.split('\\t')[1])]\n",
    "            word_pairs.append(curr)\n",
    "\n",
    "        return zip(*word_pairs)\n",
    "\n",
    "    #tokenize words and pad sentences\n",
    "    def tokenize(self, lang):\n",
    "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "        lang_tokenizer.fit_on_texts(lang)\n",
    "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, lang_tokenizer\n",
    "\n",
    "    def load_dataset(self, path, num_examples=None):\n",
    "        targ_lang, inp_lang = self.create_dataset(path, num_examples)\n",
    "        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n",
    "        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
    "\n",
    "    def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE, file_path):\n",
    "        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(file_path, num_examples)\n",
    "        \n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
    "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
    "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T15:59:02.067496Z",
     "start_time": "2021-03-22T15:58:58.922437Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "file_path = r'C:\\Users\\urvi3\\Desktop\\Projects\\nmt\\fra.txt'\n",
    "num_examples = 50000\n",
    "\n",
    "dataset_creator = NMTDataset('en-fra')\n",
    "train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T15:51:42.091679Z",
     "start_time": "2021-03-22T15:51:42.082738Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "max_length_input = example_input_batch.shape[1]\n",
    "max_length_output = example_target_batch.shape[1]\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "steps_per_epoch = num_examples//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T15:52:09.566430Z",
     "start_time": "2021-03-22T15:52:09.552436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length_french, max_length_english, vocab_size_french, vocab_size_english\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19, 11, 10220, 5893)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"max_length_french, max_length_english, vocab_size_french, vocab_size_english\")\n",
    "max_length_input, max_length_output, vocab_inp_size, vocab_tar_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T15:53:41.552445Z",
     "start_time": "2021-03-22T15:53:41.535456Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
    "        return output, h, c\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T15:56:45.397565Z",
     "start_time": "2021-03-22T15:56:45.384599Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.attention_type = attention_type\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
    "                                                                  None, self.batch_sz*[max_length_input], self.attention_type)\n",
    "        self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "\n",
    "    def build_rnn_cell(self, batch_sz):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
    "                                      self.attention_mechanism, attention_layer_size=self.dec_units)\n",
    "        return rnn_cell\n",
    "\n",
    "    def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='bahdanau'):\n",
    "    # typ: Which sort of attention (Bahdanau, Luong)\n",
    "\n",
    "        if(attention_type=='bahdanau'):\n",
    "            return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "        else:\n",
    "            return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "    def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "        return decoder_initial_state\n",
    "\n",
    "\n",
    "    def call(self, inputs, initial_state):\n",
    "        x = self.embedding(inputs)\n",
    "        outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T15:58:12.477913Z",
     "start_time": "2021-03-22T15:58:12.458928Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T16:43:18.973445Z",
     "start_time": "2021-03-22T16:43:18.954496Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T16:43:53.771271Z",
     "start_time": "2021-03-22T16:43:53.749074Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "\n",
    "\n",
    "        dec_input = targ[ : , :-1 ] \n",
    "        real = targ[ : , 1: ]         \n",
    "\n",
    "        # Set the AttentionMechanism object with encoder_outputs\n",
    "        decoder.attention_mechanism.setup_memory(enc_output)\n",
    "\n",
    "        # Create AttentionWrapperState as initial_state for decoder\n",
    "        decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "        pred = decoder(dec_input, decoder_initial_state)\n",
    "        logits = pred.rnn_output\n",
    "        loss = loss_function(real, logits)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T19:45:10.814813Z",
     "start_time": "2021-03-22T16:44:05.092746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.5775\n",
      "Epoch 1 Batch 100 Loss 0.6505\n",
      "Epoch 1 Batch 200 Loss 0.5514\n",
      "Epoch 1 Batch 300 Loss 0.4836\n",
      "Epoch 1 Batch 400 Loss 0.5569\n",
      "Epoch 1 Batch 500 Loss 0.4525\n",
      "Epoch 1 Batch 600 Loss 0.5357\n",
      "Epoch 1 Loss 0.4695\n",
      "Time taken for 1 epoch 921.2499451637268 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.3940\n",
      "Epoch 2 Batch 100 Loss 0.3917\n",
      "Epoch 2 Batch 200 Loss 0.2942\n",
      "Epoch 2 Batch 300 Loss 0.3191\n",
      "Epoch 2 Batch 400 Loss 0.3988\n",
      "Epoch 2 Batch 500 Loss 0.4047\n",
      "Epoch 2 Batch 600 Loss 0.3052\n",
      "Epoch 2 Loss 0.3146\n",
      "Time taken for 1 epoch 860.4996886253357 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2384\n",
      "Epoch 3 Batch 100 Loss 0.2381\n",
      "Epoch 3 Batch 200 Loss 0.2187\n",
      "Epoch 3 Batch 300 Loss 0.3022\n",
      "Epoch 3 Batch 400 Loss 0.2956\n",
      "Epoch 3 Batch 500 Loss 0.2216\n",
      "Epoch 3 Batch 600 Loss 0.3083\n",
      "Epoch 3 Loss 0.2328\n",
      "Time taken for 1 epoch 927.1676163673401 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1976\n",
      "Epoch 4 Batch 100 Loss 0.1894\n",
      "Epoch 4 Batch 200 Loss 0.1875\n",
      "Epoch 4 Batch 300 Loss 0.2886\n",
      "Epoch 4 Batch 400 Loss 0.1841\n",
      "Epoch 4 Batch 500 Loss 0.2133\n",
      "Epoch 4 Batch 600 Loss 0.2141\n",
      "Epoch 4 Loss 0.1872\n",
      "Time taken for 1 epoch 977.0835242271423 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1779\n",
      "Epoch 5 Batch 100 Loss 0.1947\n",
      "Epoch 5 Batch 200 Loss 0.2222\n",
      "Epoch 5 Batch 300 Loss 0.1499\n",
      "Epoch 5 Batch 400 Loss 0.2204\n",
      "Epoch 5 Batch 500 Loss 0.2257\n",
      "Epoch 5 Batch 600 Loss 0.2094\n",
      "Epoch 5 Loss 0.1605\n",
      "Time taken for 1 epoch 1019.3401074409485 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1388\n",
      "Epoch 6 Batch 100 Loss 0.1718\n",
      "Epoch 6 Batch 200 Loss 0.1352\n",
      "Epoch 6 Batch 300 Loss 0.2334\n",
      "Epoch 6 Batch 400 Loss 0.1962\n",
      "Epoch 6 Batch 500 Loss 0.2550\n",
      "Epoch 6 Batch 600 Loss 0.2352\n",
      "Epoch 6 Loss 0.1429\n",
      "Time taken for 1 epoch 1080.789941072464 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1315\n",
      "Epoch 7 Batch 100 Loss 0.1700\n",
      "Epoch 7 Batch 200 Loss 0.1433\n",
      "Epoch 7 Batch 300 Loss 0.1323\n",
      "Epoch 7 Batch 400 Loss 0.1416\n",
      "Epoch 7 Batch 500 Loss 0.1472\n",
      "Epoch 7 Batch 600 Loss 0.1439\n",
      "Epoch 7 Loss 0.1275\n",
      "Time taken for 1 epoch 1186.9669880867004 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0882\n",
      "Epoch 8 Batch 100 Loss 0.1154\n",
      "Epoch 8 Batch 200 Loss 0.1478\n",
      "Epoch 8 Batch 300 Loss 0.1804\n",
      "Epoch 8 Batch 400 Loss 0.1383\n",
      "Epoch 8 Batch 500 Loss 0.1277\n",
      "Epoch 8 Batch 600 Loss 0.1786\n",
      "Epoch 8 Loss 0.1187\n",
      "Time taken for 1 epoch 1332.7971680164337 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1541\n",
      "Epoch 9 Batch 100 Loss 0.1047\n",
      "Epoch 9 Batch 200 Loss 0.1191\n",
      "Epoch 9 Batch 300 Loss 0.1780\n",
      "Epoch 9 Batch 400 Loss 0.1785\n",
      "Epoch 9 Batch 500 Loss 0.1387\n",
      "Epoch 9 Batch 600 Loss 0.1601\n",
      "Epoch 9 Loss 0.1109\n",
      "Time taken for 1 epoch 1252.6367769241333 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0663\n",
      "Epoch 10 Batch 100 Loss 0.1101\n",
      "Epoch 10 Batch 200 Loss 0.1174\n",
      "Epoch 10 Batch 300 Loss 0.0878\n",
      "Epoch 10 Batch 400 Loss 0.1242\n",
      "Epoch 10 Batch 500 Loss 0.1456\n",
      "Epoch 10 Batch 600 Loss 0.1143\n",
      "Epoch 10 Loss 0.1045\n",
      "Time taken for 1 epoch 1307.1653406620026 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch,\n",
    "                                                       batch_loss.numpy()))\n",
    " \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T19:55:44.832005Z",
     "start_time": "2021-03-22T19:55:44.820037Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_sentence(sentence):\n",
    "    sentence = dataset_creator.preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_length_input,\n",
    "                                                          padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
    "    end_token = targ_lang.word_index['<end>']\n",
    "\n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
    "    return outputs.sample_id.numpy()\n",
    "\n",
    "def translate(sentence):\n",
    "    result = evaluate_sentence(sentence)\n",
    "    print(result)\n",
    "    result = targ_lang.sequences_to_texts(result)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T19:55:45.380386Z",
     "start_time": "2021-03-22T19:55:45.285133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63 973   4   3]]\n",
      "Input: bonjour.\n",
      "Predicted translation: ['good morning . <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'bonjour.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
